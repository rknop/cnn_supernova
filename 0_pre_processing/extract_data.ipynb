{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to extract data from .gif files into .npy arrays\n",
    "### Feb 27, 2020 \n",
    "##### Venkitesh Ayyar (vpa@lbl.gov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "## modules for parallelization of python for loop\n",
    "from multiprocessing import Pool\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code structure:\n",
    "- Read *summary_label_files.csv* and extract as a DataFrame\n",
    "- Shuffle contents and extract file name for each index\n",
    "- Read the data into a numpy array for each .gif file\n",
    "- Join the 3 arrays for temp,srch,diff\n",
    "- Append this numpy array to a list\n",
    "- Stack this list to create numpy array\n",
    "- Do the same for ID and Label(Y)\n",
    "- Write all to .npy files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_get_df():\n",
    "    '''\n",
    "    Function to get Dataframe and shuffle entries\n",
    "    3 modes: \n",
    "    - full: Get a big dataframe, shuffling all entries\n",
    "    - split: Split dataframe into :srch,temp,diff and shuffle each and return list of 3 dataframes\n",
    "    '''\n",
    "    \n",
    "    data_dir='/global/project/projectdirs/dasrepo/vpa/supernova_cnn/data/gathered_data/'\n",
    "    fname1=data_dir+'summary_label_files.csv'\n",
    "    df=pd.read_csv(fname1,sep=',',comment='#')\n",
    "    \n",
    "    ### Print summary of data\n",
    "    print(df.shape)\n",
    "    num_samples=df.shape[0]\n",
    "    num_sig,num_bkgnd=df[df.Label==1].shape[0],df[df.Label==0].shape[0]\n",
    "    print(\"Proportion of Signal-Background: {0}-{1}\\nProportion of Signal: {2}\".format(num_sig,num_bkgnd,num_sig*1.0/(num_sig+num_bkgnd)))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def f_get_data(df,idx_arr):\n",
    "    '''\n",
    "    Function to get data from .gif files into index, images, labels.\n",
    "    Uses Dataframe and an array of indices to extract data.\n",
    "    Uses matplotlib.pyplot.imread \n",
    "    '''\n",
    "    \n",
    "    combined_imgs_lst=[]\n",
    "    label_lst=[]\n",
    "    \n",
    "    ### Shorten datafame to the IDs in idx array. This speeds up the search for values.\n",
    "    df=df[df.ID.isin(idx_arr)]\n",
    "\n",
    "    ### Iterate over IDs, stacking 3 numpy arrays (temp,srch,diff) for each\n",
    "    for idx in idx_arr:\n",
    "        try: \n",
    "            ### Extract the 3 images and create stacked numpy array\n",
    "            file_list=[df[(df.ID==idx) & (df.filename.str.startswith(strg))]['file path'].values[0] for strg in ['temp','srch','diff']]\n",
    "            \n",
    "            img=np.dstack([plt.imread(fle) for fle in file_list]) ## Create stacked numpy array of 3 images\n",
    "            combined_imgs_lst.append(img)             ## Append image to list\n",
    "\n",
    "            ### Extract the first label\n",
    "            label=[df[(df.ID==idx) & (df.filename.str.startswith(strg))]['Label'].values[0] for strg in ['temp','srch','diff']]\n",
    "            ## Check that all 3 images have same label\n",
    "            assert all(x==label[0] for x in label), \"Labels for temp,srch,diff are not identical %\"%(label)\n",
    "            label_lst.append(label[0])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"Found exception\",e,'for index',idx)\n",
    "            raise SystemError\n",
    "#             pass\n",
    "    \n",
    "    ### Stack the combined image list\n",
    "    images=np.stack(combined_imgs_lst,axis=0)\n",
    "#     print(images.shape)\n",
    "    \n",
    "    ### Extract labels\n",
    "    labels = np.array(label_lst)\n",
    "    \n",
    "    ### Store the ID of the dataframe\n",
    "    idx=idx_arr[:]\n",
    "    \n",
    "    return idx,images,labels\n",
    "\n",
    "\n",
    "def f_save_files(idx,img,label,name_prefix,save_location):\n",
    "    '''\n",
    "    Save the ID, image and label in 3 .npy files\n",
    "    '''\n",
    "    f1,f2,f3=[name_prefix+i for i in ['idx','x','y']]\n",
    "    \n",
    "    for fname,data in zip([f1,f2,f3],[idx,img,label]):\n",
    "        np.save(save_location+fname,data)\n",
    "\n",
    "\n",
    "def f_concat_temp_files(num_batches,save_location):\n",
    "    '''\n",
    "    Function to concatenate temp files to creat the full file.\n",
    "    Steps: get data from temp files, stack numpy arrays and delete temp files\n",
    "    '''\n",
    "    \n",
    "    for count in np.arange(num_batches):\n",
    "        prefix='temp_data_%s'%(count)\n",
    "        f1,f2,f3=[prefix+ii+'.npy' for ii in ['_x','_y','_idx']]\n",
    "        \n",
    "        xs,ys,idxs=np.load(save_location+f1),np.load(save_location+f2),np.load(save_location+f3)\n",
    "    \n",
    "        ### Join arrays to create large array\n",
    "#         print(xs.shape,count+1,\"out of \",num_batches)\n",
    "\n",
    "        if count==0:\n",
    "            x=xs;y=ys;idx=idxs\n",
    "        else:\n",
    "            x = np.vstack((x,xs))\n",
    "            y = np.concatenate((y,ys))\n",
    "            idx= np.concatenate((idx,idxs))\n",
    "            \n",
    "        for fname in [f1,f2,f3]: os.remove(save_location+fname) # Delete temp file\n",
    "    print(\"Deleted temp files\")\n",
    "        \n",
    "    return x,y,idx\n",
    "\n",
    "def f_write_temp_files(count,batch_size,save_location):\n",
    "    '''\n",
    "    Function to write temporary files\n",
    "    Arguments: count: index of idx array and batch_size : size of batch\n",
    "    Takes in indices count*batch_size -> (count+1)*batch_size\n",
    "    Can be used to run in parallel\n",
    "    '''\n",
    "    t3=time.time()\n",
    "    idx,img,label=f_get_data(df,idx_arr[count*batch_size:(count+1)*batch_size])\n",
    "    prefix='temp_data_{0}_'.format(count)\n",
    "    f_save_files(idx,img,label,prefix,save_location)\n",
    "    t4=time.time()\n",
    "    print(\"Extraction time for count \",count,\":\",t4-t3)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2696889, 4)\n",
      "Proportion of Signal-Background: 1334613-1362276\n",
      "Proportion of Signal: 0.49487131283489977\n",
      "Setup time 7.150082588195801\n",
      "10000 1000 10\n",
      "Number of temp files:  10\n",
      "Extraction time for count  8 : 28.160730123519897\n",
      "Extraction time for count  7 : 30.241002082824707\n",
      "Extraction time for count  6 : 30.35751223564148\n",
      "Extraction time for count  3 : 31.371824979782104\n",
      "Extraction time for count  1 : 31.953946352005005\n",
      "Extraction time for count  9 : 32.068790912628174\n",
      "Extraction time for count  2 : 32.097533226013184\n",
      "Extraction time for count  5 : 33.636959075927734\n",
      "Extraction time for count  4 : 33.83048462867737\n",
      "Extraction time for count  0 : 34.55270743370056\n",
      "Time for concatenation of file: 0.49846720695495605\n",
      "total time 43.222331285476685\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    procs=10\n",
    "    batch_size=1000\n",
    "    save_location='/global/project/projectdirs/dasrepo/vpa/supernova_cnn/data/gathered_data/temp_data/'\n",
    "    \n",
    "    print('batch size {0}, and processes {1}'.format(batch_size,procs))\n",
    "    \n",
    "    t1=time.time()\n",
    "    ### Get Dataframe with indices and file names\n",
    "    df=f_get_df()\n",
    "    ### Uncomment the line below if you want to run a slice of the dataset\n",
    "    ###df=df.head(300000) ### make sure you take multiples of 3 so that all 3 files for each ID are taken\n",
    "    \n",
    "    ### Get list of IDs. Each ID has a srch,temp,diff file\n",
    "    idx_arr=np.unique(df.ID.values)\n",
    "    #### Shuffle IDs\n",
    "    np.random.seed(37)\n",
    "    np.random.shuffle(idx_arr) \n",
    "    np.save(save_location+'initial_idx_arr.npy',idx_arr)  ### Save the ID file for final comparison\n",
    "    \n",
    "    t2=time.time()\n",
    "    print(\"Setup time (reading the Datarame) \",t2-t1)\n",
    "    \n",
    "    data_size=idx_arr.shape[0]\n",
    "    batch_size=min(batch_size,data_size) ### Fix for large batch size\n",
    "    num_batches=int(np.ceil(data_size/batch_size))\n",
    "    print(data_size,batch_size,num_batches)\n",
    "    print(\"Number of temp files: \",num_batches)\n",
    "    \n",
    "    ### Save batches of samples to temp files \n",
    "    ##### This part is parallelized\n",
    "    with Pool(processes=procs) as p:\n",
    "        ## Fixing the last 2 arguments of the function. The map takes only functions with one argument\n",
    "        f_temp_func=partial(f_write_temp_files,batch_size=batch_size,save_location=save_location)\n",
    "        ### Map the function for each batch. This is the parallelization step\n",
    "        p.map(f_temp_func, np.arange(num_batches))\n",
    "    \n",
    "    t5=time.time()\n",
    "    \n",
    "    ### Concatenate temp files\n",
    "    t6=time.time()\n",
    "    img,label,idx=f_concat_temp_files(num_batches,save_location)\n",
    "    t7=time.time()\n",
    "    print(\"Time for concatenation of file:\",t7-t6)\n",
    "    \n",
    "    ### Save concatenated files\n",
    "    f_save_files(idx,img,label,'full_',save_location)\n",
    "    t8=time.time()\n",
    "    \n",
    "    print(\"total time\",t8-t1)\n",
    "    \n",
    "    ######################################################\n",
    "    ######################################################\n",
    "    ### Check if the concatenated index arrays are identical\n",
    "    ## this is a minor concatenation check. The full check is performed in a jupyter notebook.\n",
    "    a1=np.load(save_location+'initial_idx_arr.npy')\n",
    "    a2=np.load(save_location+'full_idx.npy')\n",
    "\n",
    "    assert np.array_equal(a1,a2),\"The index arrays after concatenation are not identical\"\n",
    "    print(\"ID arrays identical? \",np.array_equal(a1,a2))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "March 6, 2020\n",
    "Time for extraction: \n",
    "On cori-haswell with 32 procs, batch size 1000 -> 2769s\n",
    "On cori-knl with 68 procs, batch size 100 -> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check by reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-257628eb69a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# fname='/global/project/projectdirs/dasrepo/vpa/supernova_cnn/data/gathered_data/temp_data/temp_x.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0ma1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# a2=np.load()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "fname='/global/project/projectdirs/dasrepo/vpa/supernova_cnn/data/gathered_data/temp_data/full_x.npy'\n",
    "# fname='/global/project/projectdirs/dasrepo/vpa/supernova_cnn/data/gathered_data/input_npy_files/full_x.npy'\n",
    "\n",
    "# fname='/global/project/projectdirs/dasrepo/vpa/supernova_cnn/data/gathered_data/temp_data/temp_x.npy'\n",
    "\n",
    "a1=np.load(fname)\n",
    "print(a1.shape)\n",
    "# a2=np.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname='/global/project/projectdirs/dasrepo/vpa/supernova_cnn/data/gathered_data/temp_data/full_x.npy'\n",
    "a1=np.load(fname)\n",
    "fname='/global/project/projectdirs/dasrepo/vpa/supernova_cnn/data/gathered_data/temp_data/fullx2.npy'\n",
    "a2=np.load(fname)\n",
    "\n",
    "print(a1.shape,a2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_py3",
   "language": "python",
   "name": "v_jpt_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
