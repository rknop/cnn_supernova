{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model\n",
    "Read stored model and test on custom dataset\\\n",
    "Sep 1, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import pandas as pd\n",
    "import time\n",
    "import argparse\n",
    "import sys\n",
    "import subprocess as sp\n",
    "import pickle\n",
    "import yaml\n",
    "\n",
    "## M-L modules\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks  # or tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "## modules from other files\n",
    "from models import *\n",
    "from utils import dataset, cnn_model, f_get_data, f_load_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir='/global/cfs/cdirs/dasrepo/vpa/supernova_cnn/data/results_data/results/test_with_train_data/'\n",
    "data_dir='/global/project/projectdirs/dasrepo/vpa/supernova_cnn/data/gathered_data/input_npy_files/'\n",
    "prefix='full'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file name /global/project/projectdirs/dasrepo/vpa/supernova_cnn/data/gathered_data/input_npy_files/full_x.npy\n",
      "Size of entire dataset is :  898963\n",
      "449481 494429 539377\n",
      "\n",
      "Data shapes: Train (449481, 51, 51, 3), Validation (44948, 51, 51, 3), Test (89896, 51, 51, 3)\n",
      "\n",
      "Time taken to read files 0.0027387142181396484\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### Read data from files \n",
    "data_dict=f_get_data(prefix,data_dir,pre_norm=False)\n",
    "\n",
    "t1=time.time()\n",
    "\n",
    "size_data=data_dict['labels'].shape[0]\n",
    "print(\"Size of entire dataset is : \",size_data)\n",
    "#### Define the indices for training, validation and test data\n",
    "train_idx=int(0.5*size_data)\n",
    "val_idx=train_idx+int(0.05*size_data)\n",
    "test_idx=val_idx+int(0.05*size_data)\n",
    "\n",
    "print(train_idx,val_idx,test_idx)\n",
    "#### Storing arrays into train,validation, test objects and deleting the full data dictionary\n",
    "train_data=dataset('training',data_dict,start_idx=0,end_idx=train_idx)\n",
    "val_data=dataset('validation',data_dict,start_idx=train_idx,end_idx=val_idx)\n",
    "# test_data=dataset('test',data_dict,start_idx=val_idx,end_idx=test_idx)\n",
    "\n",
    "test_data=dataset('test',data_dict,start_idx=int(0.2*size_data),end_idx=int(0.3*size_data))\n",
    "\n",
    "\n",
    "del data_dict\n",
    "print(\"\\nData shapes: Train {0}, Validation {1}, Test {2}\\n\".format(train_data.x.shape,val_data.x.shape,test_data.x.shape))\n",
    "\n",
    "t2=time.time()\n",
    "print(\"Time taken to read files\",t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('train_ids.train',train_data.id)  ### Needed only if you want to use train IDs for Random Forest.\n",
    "# Otherwise ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89896,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=test_data\n",
    "data.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(89896, 51, 51, 3)\n",
      "89896/89896 [==============================] - 8s 87us/sample\n",
      "8\n",
      "(89896, 51, 51, 3)\n",
      "89896/89896 [==============================] - 12s 128us/sample\n",
      "9\n",
      "(89896, 51, 51, 3)\n",
      "89896/89896 [==============================] - 15s 167us/sample\n",
      "16\n",
      "(89896, 51, 51, 3)\n",
      "89896/89896 [==============================] - 7s 82us/sample\n",
      "333\n",
      "(89896, 51, 51, 3)\n",
      "89896/89896 [==============================] - 8s 92us/sample\n"
     ]
    }
   ],
   "source": [
    "# model_name='3'\n",
    "for model_name in [str(i) for i in [3,8,9,16,333]]:\n",
    "    print(model_name)\n",
    "    ### Define Object for cnn_model\n",
    "    Model=cnn_model(model_name,model_save_dir)\n",
    "\n",
    "    ### Read stored model and history\n",
    "    Model.f_load_model_history()\n",
    "\n",
    "    #################################\n",
    "    ### Test model ###\n",
    "    Model.f_test_model(data)\n",
    "\n",
    "    ## Save prediction array and labels array\n",
    "    Model.f_save_predictions(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_py3",
   "language": "python",
   "name": "v_jpt_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
